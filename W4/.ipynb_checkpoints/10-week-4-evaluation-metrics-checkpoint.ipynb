{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In machine learning, especially supervised learning, there's high importance to answer these two questions:\n",
    "\n",
    "1. How well is the model doing?\n",
    "2. How do we improve the model based on those metrics?\n",
    "\n",
    "Hence, we need to know about evaluation metrics and how they lead us to test and improve our model.\n",
    "\n",
    "<img src=\"../python-for-data-science/assets/img/outline-eval.png\" width=\"50%\">\n",
    "\n",
    "Remember that for supervised learning, we have 2 problems, **regression** and **classification**. Regression returns a real value, classification returns a state. The evaluation metrics is different for each problem. We will focus more on classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Consfusion matrix, in machine learning, is a table layout that visualize the performance of a model. For classification, there are 4 possibilities.\n",
    "\n",
    "1. A positive example that is classified as positive (**True Positive**)\n",
    "2. A positive example that is classified as negative (**False Negative**)\n",
    "3. A negative example that is classified as positive (**False Positive**)\n",
    "4. A negetaive example that is classified as negative (**True Negative**)\n",
    "\n",
    "We can visualize this as a confusion matrix to tells how many each possibility happen.\n",
    "\n",
    "<img src=\"../python-for-data-science/assets/img/confusion-matrix.png\" width=\"40%\">\n",
    "\n",
    "Now, we have 2 cases, **sick patient** and **detecting spam**. There are $10,000$ patients in total for sick patient case and $1,000$ emails for spam detection. We've built the model, and got below performances.\n",
    "\n",
    "<img src=\"../python-for-data-science/assets/img/confusion-case-medical.png\" width=\"40%\"/>\n",
    "<img src=\"../python-for-data-science/assets/img/confusion-case-spam.png\" width=\"40%\"/>\n",
    "\n",
    "Sometimes, in the literature, we'll see False Positive and False Negative as Type 1 and Type 2 Errors. Here's the correspondence:\n",
    "- In the medical example, when we misclassified a healthy patient as sick, it's a **Type 1 Error** or **False Positive**\n",
    "- In the medical example, when we misclassified a sick patient as healthy, it's a **Type 2 Error** or **False Negative**\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "One of the ways to measure how good the model is **accuracy**. Accuracy answer this kind of question:\n",
    "> Out of all the patients, how many did we classify correctly?\n",
    "\n",
    "We can calculate the accuracy of the model using this equation\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{\\text{True Positive} + \\text{True Negative}}{\\text{Total}}\n",
    "$$\n",
    "\n",
    "Then, we can look at the confusion matrix for both medical and spam examples.\n",
    "1. How much accuracy do we get for medical model?\n",
    "2. How much accuracy do we get for spam model?\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "Sometimes, accuracy won't work to measure how good our model. Take an example of **fraud detection** problem. Say, we have a dataset with `214500` are good transactions and `500` transactions are fraud/bad. Now, what state do the model need to be at for acquiring accuracy of 99%? Seems weird, right?\n",
    "\n",
    "1. In medical example, between False Negative and False Positive, which one is worse?\n",
    "2. In spam example, between False Negative and False Positive, which one is worse?\n",
    "\n",
    "This kind of problem address us to use other metrics, namely **precision** and **recall**.\n",
    "**Precision** answer this kind of question, out of all *positively classified examples*, how many of them are positive examples? For **Recall**, it answer question like, out of all *positive examples*, how many of them do we correctly classified as *positive*? Below equations may help you to calculate precision and recall.\n",
    "\n",
    "$$\n",
    "precision = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive}} \\\\\n",
    "recall = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Score\n",
    "\n",
    "Now we have accuracy, precision, and recall for our metrics. But, we don't want to carry around a lot of metrics, and we may wonder is there any metrics which somehow describe both precision and recall in a sensible way? There is, it's F1 score.\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "F1 score is the *harmonic mean* between precision and recall. It has the following equation\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "What if we want to choose precision rather than recall, or otherwise? We can use more general score, namely, $F_\\beta$ score.\n",
    "\n",
    "### F-beta score\n",
    "\n",
    "In more general equation, `beta` controls what metrics we focus on, precision or recall. If we set a small `beta`, we then focus more on precision. If we set a higher `beta`, we then focus more on recall. F-beta score has the following equation.\n",
    "\n",
    "$$\n",
    "\\text{F-beta Score} = (1-\\beta^2) \\cdot \\frac{precision \\cdot recall}{\\beta^2 \\cdot precision + recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC\n",
    "\n",
    "Receiving Operator Characteristic, or ROC for short, is another type of metrics which illustrate and compare True Positive Rate against False Positive Rate. It's pretty useful even for multiclass classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
