{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Clustering\n",
    "\n",
    "Previously, we have covered K-Means where we set the number of cluster `K`, generate centroids randomly, and repeat until we have optimal clusters with minimum distance for a point to the corresponding cluster. Since most clustering depend on the distance, hence **feature scaling** is necessary step in preprocessing before fitting a cluster algorithm.\n",
    "\n",
    "The most used feature scaling methods are **Standardizaton** (z-scores scaling) and **Normalization** (min-max scaling). We will mostly use the first step in clustering and the latter for scaling the color of image.\n",
    "\n",
    "## Beyond K-Means\n",
    "\n",
    "You may have some concerns with K-Means. These concerns included:\n",
    "\n",
    "1. Concern: The random placement of the centroids may lead to non-optimal solutions.\n",
    "\n",
    "**Solution**: Run the algorithm multiple times and choose the centroids that create the smallest average distance of the points to the centroids.\n",
    "\n",
    "2. Concern: Depending on the scale of the features, you may end up with different groupings of your points.\n",
    "\n",
    "**Solution**: Scale the features using Standardizing, which will create features with mean 0 and standard deviation 1 before running the k-means algorithm.\n",
    "\n",
    "3. Concern: Depending on the dataset, K-Means may not work on a two-ring dataset or crescent dataset because its nature of circle/spherical along centroids.\n",
    "\n",
    "**Solution**: Consider other clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "A **hierarchical clustering** is an unsupervised clustering method which involves creating clusters in a hierarchy form, from top to bottom, like a tree. This tree is called **dendrogram**. Below are the steps how hierarchical clustering works:\n",
    "\n",
    "1. Start with assuming each point is already a cluster.\n",
    "2. Compute the proximity for each point\n",
    "3. Merge closest clusters into one.\n",
    "4. Repeat from step 2 until a single cluster remains.\n",
    "\n",
    "There some types for calculating the proximity distance between each cluster:\n",
    "- **Single link(age)**: the distance between two clusters is defined as the **shortest** distance between two points in each cluster.\n",
    "- **Complete link(age)**: the distance between two clusters is defined as the **longest** distance between two points in each cluster.\n",
    "- **Average link(age)**: the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster.\n",
    "- **Ward's method**: this is similar with average linkage, but it use the squared root of the distance instead of averaging. (The default method in hierarchical in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Implementation\n",
    "\n",
    "Here, we will use hierarchical clustering feature provided in both `sklearn` and `scipy`. We also consider `scipy` since it has dendrogram for hierarchy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Clustering\n",
    "\n",
    "DBSCAN, or Density Based Spatial Clustering of Applications with Noise, groups together points that are closely packed together, hence yields high density spatially. DBSCAN works slightly different with other clustering methods. It because they don't consider every points are part of the clusters. These points are labeled as noise.\n",
    "\n",
    "Two hyperparameters need to be set when implementing DBSCAN is `epsilon` and `MinPts`.\n",
    "- `epsilon`: determines search distance around the **core point**.\n",
    "- `MinPts`: minimum number of points required to form a density cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Clustering Implementation\n",
    "\n",
    "We will use `sklearn` to implement DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
