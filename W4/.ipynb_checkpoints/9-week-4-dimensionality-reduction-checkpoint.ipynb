{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Other than clustering, **dimensionality reduction** is one of unsupervised learning technique that aims to transform data, not group them together. The goal of all dimensionality reduction algorithm is **retain the informative parts of the data but with less of it**. For example, the answer for below question\n",
    "\n",
    "> **what do you have for breakfast today?**\n",
    "<br/>So I was running late, and..\n",
    "<br/>I stopped by the coffe shop, and..\n",
    "<br/>I had a coffee, and then..\n",
    "<br/>I grabbed a pastry.\n",
    "\n",
    "Can be reduced into\n",
    "\n",
    "> I had a coffe and pastry.\n",
    "\n",
    "We reduce the dimensionality of dataset yielding a new data with **latent features**. Latent features are features that are not directly observed, but underlies other features that are observed.\n",
    "\n",
    "There are some ways to create a latent features. The most simple way is we can choose some features among others that tells us more information based on the context of dataset and case we deal with. But with this method, we will a lot of information becuase we simply drop and not use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "With PCA, we can create latent features by combining all features into latent features, a few features preserving enough information from dataset.\n",
    "\n",
    "Principal Component Analysis utilise a linear combination of observed features which form **principal components**. Principal components are linear combination of the original features in a dataset that aim to retain the most information in the original data. These principal components can be thought in the same way with latent features.\n",
    "\n",
    "![](../python-for-data-science/assets/img/pca-example.png)\n",
    "\n",
    "There are some properties of principal components:\n",
    "- Each components captures the largets amounts of variance left in the data. With this, we **don't lose much amount of information from data**. It means, we try to find a line the reduces the distance of points to the component for all the points (just like regression)\n",
    "- The created components are **orthogonal** to another. It means, when there are more components, the additional components will be orthogonal to one another. These orthogonal principal components will benefit since we don't have dependent features in our new dataset.\n",
    "\n",
    "More details about PCA can be found below:\n",
    "- [Why are PCs bound to be orthogonal](https://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)\n",
    "- [Making sense of PCA](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
