{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is commonly used with high dimensional data.  One type of high dimensional data is images. In this example, I will show you a similar use case using images of numbers.\n",
    "\n",
    "A classic example of working with image data is the MNIST dataset, which was open sourced in the late 1990s by researchers across Microsoft, Google, and NYU.  I have linked more information about the dataset below this video in the workbook:  http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "To get started, let's import the libraries we will need for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 42,000 images in the training dataset, and another 28,000 images in a test set.  Let's read in the datasets and see what they look like.  If you would like to follow along with me, you can open the workspace below this video in another window and work along with me.  Let's take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data may take a bit of time, but once it is loaded, we will focus on the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a header and the describe of the data, we can see that the data has a label column with what digit was created, and the pixels for each part of the image are the features.  The values for the pixels are between 0 and 255.  You can find an article on the pixel values in the article below in the notebook. https://homepages.inf.ed.ac.uk/rbf/HIPR2/value.htm\n",
    "\n",
    "Given what we know about the data, let's split the label off and put the images in their own matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the labels to a Pandas series target\n",
    "y = train['label']\n",
    "# Drop the label feature\n",
    "X = train.drop(\"label\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y, color = sns.color_palette()[0]); #a very quick look at the data shows that all the labels appear roughly 4k times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what a few of the numbers look like with this little helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(num_images):\n",
    "    '''\n",
    "    This function plots the num_images provided of MNIST dataset. \n",
    "    \n",
    "    INPUT: num_images - int - The number of images you would like to view.\n",
    "           Mod 10 of num_images should be 0 and it should be fewer than 101 images.\n",
    "    OUTPUT: A figure with the images shown for the training data.\n",
    "    '''\n",
    "    if num_images % 10 == 0 and num_images <= 100:\n",
    "        for digit_num in range(0,num_images): \n",
    "            plt.subplot(num_images/10,10,digit_num+1) #create subplots\n",
    "            mat_data = X.iloc[digit_num].as_matrix().reshape(28,28)  #reshape images\n",
    "            plt.imshow(mat_data) #plot the data\n",
    "            plt.xticks([]) #removes numbered labels on x-axis\n",
    "            plt.yticks([]) #removes numbered labels on y-axis\n",
    "    else:\n",
    "        print('That is not the right input, please read the docstring before continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these look pretty easy to identify, and others not so much... I wrote another helper function to be able to see each of the first 50 images of any one type of number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_by_digit(digit_to_see):\n",
    "    '''\n",
    "    This function plots the digits_to_see provided of MNIST dataset. \n",
    "    \n",
    "    INPUT: digits_to_see - int - A number between 0 and 9 of what you want to see.\n",
    "    OUTPUT: A figure with the images shown for the training data.\n",
    "    '''\n",
    "    if digit_to_see in list(range(10)):\n",
    "        indices = np.where(y == digit_to_see) # pull indices for num of interest\n",
    "        for digit_num in range(0,50): \n",
    "            plt.subplot(5,10, digit_num+1) #create subplots\n",
    "            mat_data = X.iloc[indices[0][digit_num]].as_matrix().reshape(28,28)  #reshape images\n",
    "            plt.imshow(mat_data) #plot the data\n",
    "            plt.xticks([]) #removes numbered labels on x-axis\n",
    "            plt.yticks([]) #removes numbered labels on y-axis   \n",
    "    else:\n",
    "        print('That is not the right input, please read the docstring before continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_by_digit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these ones are pretty wild looking...  One common way to use PCA is to reduce the dimensionality of high dimensionality data that you want to use for prediction, but the results seem to be overfitting (potentially because their is a lot of noise in the data.  Which can certainly be the case with image data).\n",
    "\n",
    "Let's take a first pass on creating a simple model to predict the values of the images using all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we will fit this more than once, might as well make a function\n",
    "def fit_random_forest_classifier(X, y):\n",
    "    '''\n",
    "    INPUT: names are pretty self explanatory\n",
    "    OUTPUT: none - prints the confusion matrix and accuracy\n",
    "    '''\n",
    "    #First let's create training and testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #We could grid search and tune, but let's just fit a simple model to see how it does\n",
    "    #instantiate\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "\n",
    "    #fit\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #predict\n",
    "    y_preds = clf.predict(X_test)\n",
    "\n",
    "    #score\n",
    "    mat = confusion_matrix(y_test, y_preds)\n",
    "    print(mat)\n",
    "    print(sns.heatmap(mat, annot=True, cmap='bwr', linewidths=.5))\n",
    "    acc = accuracy_score(y_test, y_preds)\n",
    "    print(acc)\n",
    "    return acc\n",
    "    \n",
    "fit_random_forest_classifier(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model does pretty well on the test set using all of the data, let's see how we well a model can do with a much lower number of features.  Perhaps, we can do as well or better by reducing the noise in the original features.\n",
    "\n",
    "Working with unsupervised techniques in scikit learn follows a similar process as working with supervised techniques, but excludes predicting and scoring, and instead we just need to transform our data.  Often the fit and transform are done all in one step.\n",
    "\n",
    "1. Instantiate\n",
    "2. Fit \n",
    "3. Transform \n",
    "\n",
    "It is worth mentioning that because all of our features are on the same scale from 0 to 255, scaling isn't super necessary here.  At the same time, I usually try to create processes that are extendible without having to think and dive into the details again.  Without standardizing, this process would only work for data that was already scaled.\n",
    "\n",
    "You can use the below function to perform PCA on this dataset, but also any other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, data):\n",
    "    '''\n",
    "    Transforms data using PCA to create n_components, and provides back the results of the\n",
    "    transformation.\n",
    "    \n",
    "    INPUT: n_components - int - the number of principal components to create\n",
    "           data - the data you would like to transform\n",
    "           \n",
    "    OUTPUT: pca - the pca object created after fitting the data\n",
    "            X_pca - the transformed X matrix with new number of components\n",
    "    '''\n",
    "    X = StandardScaler().fit_transform(data)\n",
    "    pca = PCA(n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, X_pca = do_pca(2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_random_forest_classifier(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's look at the positives first... that was super fast.  But clearly two components isn't giving us enough information to clearly identify the digits.  Let's consider a few more components, and take a closer look at what we know about the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_components(X, y):\n",
    "    '''\n",
    "    plots the data in a 2 dimensional space to view separation\n",
    "    INPUT: pretty straightforward again\n",
    "    OUTPUT: none\n",
    "    '''\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]), color=plt.cm.Set1(y[i]), fontdict={'size': 15})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([]), plt.ylim([-0.1,1.1]), plt.xlim([-0.1,1.1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(X_pca[:100], y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
