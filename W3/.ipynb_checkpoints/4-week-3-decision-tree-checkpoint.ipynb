{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision Tree is one of the most popular machine learning algorithm which belongs to supervised learning type. It's a structure for decision-making where each decision leads to set of consequences or additional decisions. Decision Tree can be used for both classification and regression.\n",
    "\n",
    "## Case Study: App Recommendation Engine\n",
    "\n",
    "Say, we are about to build a recommendation for the App Store or Google Play. The task is to recommend people the app they're most likely to download, based on previous data. Below is the data we have.\n",
    "\n",
    "| Gender | Occupation | App |\n",
    "|:-----:|---------|-------|\n",
    "| F | Study | PUBG |\n",
    "| F | Work | Slack |\n",
    "| M | Work | Tinder |\n",
    "| F | Work | Slack |\n",
    "| M | Study | PUBG |\n",
    "| M | Study | PUBG |\n",
    "\n",
    "First, let's consider these questions.\n",
    "\n",
    "- For a woman who works at an office, what app do we recommend?\n",
    "- For a man who works at a factory, what app do we recommend?\n",
    "- For a girl who goes to high school, what app do we recommend?\n",
    "\n",
    "### Machine asks slightly different question\n",
    "\n",
    "- Between gender and occupation, which one seems more decisive for predicting what app will the users download?\n",
    "\n",
    "### How about data with numerical/continuous features?\n",
    "\n",
    "Consider a case of student admission with features: `Grades` and `Test`. We can ask similar question but different notion.\n",
    "\n",
    "- Between a horizontal and a vertical line, which one would cut the data better? (After making a plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "In order computers (model) to know, we introduce **Entropy**. Borrowed from physics, strictly speaking, entropy is a measure of how much freedom particles have to move around. Consider below image of state of particles.\n",
    "\n",
    "![](../python-for-=d/assets/img/entropy-particles.png)\n",
    "\n",
    "Ice will have the lowest entropy, water have medium entropy, and gas will have the highest entropy since it can move around a lot.\n",
    "\n",
    "### Entropy in Probability\n",
    "\n",
    "Imagin you have 3 buckets. Each buckets consist of 4 balls with different configuration of color.\n",
    "\n",
    "- Bucket A - 4 red balls\n",
    "- Bucket B - 3 red balls, 1 blue balls\n",
    "- Bucket C - 2 red balls, 2 blue balls\n",
    "\n",
    "Entropy measure how much balls are allowed to move around if we put them in a line? Which bucket does have the highest entropy, medium, and lowest one?\n",
    "\n",
    "## Entropy in Knowledge\n",
    "\n",
    "Using the same scenario and configuration, now consider you take one ball from those buckets. Which bucket does inform you better about the color? Which bucket does have the highest entropy and/or highest knowledge?\n",
    "\n",
    "## Entropy Formula\n",
    "\n",
    "$$\n",
    "entropy = - \\sum_{i=1}^{n}{p_i} \\cdot \\log_2 p_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "In decision tree, information gain is the change of entropy of parent nodes with the child nodes. This is where decision tree got calculated and measure how well the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Decision Trees\n",
    "\n",
    "In order to create decision trees that will generalize to new problems well, we can tune a number of different aspects about the trees. We call the different aspects of a decision tree \"hyperparameters\". These are some of the most important hyperparameters used in decision trees:\n",
    "\n",
    "### Maximum depth\n",
    "The maximum depth of a decision tree is simply the largest length between the root to a leaf. A tree of maximum length `k` can have at most $2^k$ leaves.\n",
    "\n",
    "### Minimum number of samples per leaf\n",
    "Minimum number of samples required to be at a leaf node. When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other. This will not take us too far in our process, and would be a waste of resources and time. If we want to avoid this, we can set a minimum for the number of samples we allow on each leaf.\n",
    "\n",
    "### Minimum number of samples per split\n",
    "This is the same as number of samples per leaf, but applied on any split of an internal node.\n",
    "\n",
    "### Maximum number of features\n",
    "Oftentimes, we will have too many features to build a tree. If this is the case, in every split, we have to check the entire dataset on each of the features which is very expensive. A solution for this is to limit the number of features that one looks for in each split.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
